# version: '3.8'

services:
  postgres:
    # Postges with TimescaleDB extension and PostGIS
    # Using TimescaleDB for time-series data and PostGIS for geospatial data
    image: timescale/timescaledb-ha:pg15.13-ts2.20.1
    container_name: wildlife_postgres
    environment:
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
      POSTGRES_DB: wildlife
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./init-scripts:/docker-entrypoint-initdb.d  # mount init script directory for custom initialization
    restart: always
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U user -d wildlife"]
      interval: 10s
      timeout: 5s
      retries: 5

  pgadmin:
    image: dpage/pgadmin4
    container_name: wildlife_pgadmin
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@wildlife.com
      PGADMIN_DEFAULT_PASSWORD: admin
    ports:
      - "8080:80"
    depends_on:
      - postgres


  # zookeeper:
  #   image: zookeeper:3.9
  #   container_name: wildlife_zookeeper
  #   ports:
  #     - "2181:2181"

 # enable Kafka for message brokering. new versions of Kafka do not require Zookeeper
  kafka:
    image: bitnami/kafka:3.5
    ports:
      - "9092:9092"
    environment:
    - KAFKA_CFG_NODE_ID=1
    - KAFKA_CFG_PROCESS_ROLES=broker,controller
    - KAFKA_CFG_LISTENERS=PLAINTEXT://0.0.0.0:9092,INTERNAL://0.0.0.0:9093,CONTROLLER://0.0.0.0:9094
    - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092,INTERNAL://kafka:9093
    - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,INTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT
    - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=INTERNAL
    - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@kafka:9094
    - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
    - KAFKA_KRAFT_CLUSTER_ID=sdfgR6WyR_eoC3XV2bHqMw
    volumes:
      - kafka-data:/bitnami/kafka

  kafka_consumer_worker:
    build: ./kafka/consumer_worker  # Your consumer Dockerfile path
    depends_on:
      - kafka
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9093

  kafka_producer_tester:
    build: ./kafka/consumer_worker  # Your consumer Dockerfile path
    depends_on:
      - kafka
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092

  # elasticsearch:
  #   image: docker.elastic.co/elasticsearch/elasticsearch:8.9.0
  #   container_name: wildlife_elasticsearch
  #   environment:
  #     - discovery.type=single-node
  #     - ES_JAVA_OPTS=-Xms512m -Xmx512m
  #   ports:
  #     - "9200:9200"
  #   volumes:
  #     - esdata:/usr/share/elasticsearch/data
  #   healthcheck:
  #     test: ["CMD-SHELL", "curl -fs http://localhost:9200/ || exit 1"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 5

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: wildlife_backend
    command: uvicorn main:app --host 0.0.0.0 --port 8000 --reload
    volumes:
      - ./backend:/app
    ports:
      - "8000:8000"
    environment:
      DATABASE_URL: postgresql://user:password@postgres:5432/wildlife
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      ELASTICSEARCH_URL: http://elasticsearch:9200
    depends_on:
      - postgres
      # - kafka
      # - elasticsearch

  frontend:
    build:
      context: ./frontend
      # dockerfile: Dockerfile
      dockerfile: Dockerfile.dev
    container_name: wildlife_frontend
    ports:
      # - "3000:80"
      - "3000:3000"
    depends_on:
      - backend
    environment:
      - CHOKIDAR_USEPOLLING=true  # helps with file watching on Docker volumes
    restart: always
    volumes:
      - ./frontend:/app
      - /app/node_modules
  locust:
    build: 
      context: ./locust
      dockerfile: Dockerfile
    container_name: locust
    ports:
      - "8089:8089"
    environment:
      - LOCUST_HOST=http://backend:8000  # OR http://localhost:8000 if running outside Docker
    volumes:
      - ./locust:/locust  # Make sure this points to the dir containing locustfile.py
    working_dir: /locust
    depends_on:
      - backend

volumes:
  pgdata:
  esdata:
  kafka-data:
